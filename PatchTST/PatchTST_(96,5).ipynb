{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install torchmetrics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l88TYbxEIN5L","executionInfo":{"status":"ok","timestamp":1702345769170,"user_tz":-540,"elapsed":6276,"user":{"displayName":"­최윤선 | 데이터사이언스전공 | 한양대(서울)","userId":"06275925497868952139"}},"outputId":"dbe98fd7-8a23-46b7-eac5-349ebb465fa4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchmetrics\n","  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu118)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.13.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n","Installing collected packages: lightning-utilities, torchmetrics\n","Successfully installed lightning-utilities-0.10.0 torchmetrics-1.2.1\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/인공지능프로젝트2/PatchTST')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N-sDhxJelekv","executionInfo":{"status":"ok","timestamp":1702345831405,"user_tz":-540,"elapsed":62238,"user":{"displayName":"­최윤선 | 데이터사이언스전공 | 한양대(서울)","userId":"06275925497868952139"}},"outputId":"a3a8e1db-6265-4927-a57c-5ef6c61a2528"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Colab Notebooks/인공지능프로젝트2/PatchTST"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dt07H8KvlfdV","executionInfo":{"status":"ok","timestamp":1702345831950,"user_tz":-540,"elapsed":556,"user":{"displayName":"­최윤선 | 데이터사이언스전공 | 한양대(서울)","userId":"06275925497868952139"}},"outputId":"7aa2f09e-a80b-4928-8ffe-33599a5fa725"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/인공지능프로젝트2/PatchTST\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"FpN22x6Xq5Ox","executionInfo":{"status":"ok","timestamp":1702345847643,"user_tz":-540,"elapsed":15695,"user":{"displayName":"­최윤선 | 데이터사이언스전공 | 한양대(서울)","userId":"06275925497868952139"}}},"outputs":[],"source":["import argparse\n","\n","import os\n","import time\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","from torch import optim\n","from torch.optim import lr_scheduler\n","from torch import Tensor\n","\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from typing import Callable, List, Dict, Union, Optional\n","from datetime import datetime\n","from sklearn.preprocessing import StandardScaler\n","\n","from torchmetrics.functional.regression import spearman_corrcoef\n","from torchmetrics.regression import PearsonCorrCoef\n","\n","from timefeatures import time_features\n","from exp_basic import Exp_Basic\n","from patchtst_tools import EarlyStopping, adjust_learning_rate, visual, test_params_flop\n","from metrics import metric\n","from PatchTST_backbone import PatchTST_backbone\n","from PatchTST_layers import series_decomp\n","\n","import warnings\n","\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["def read_wsdm23_file(file_path: str,) -> Dict[str, pd.DataFrame]:\n","  wsdm23_dict = np.load(file_path, allow_pickle=True).item()\n","  for df in wsdm23_dict.values():\n","    df.index.name = 'date'\n","  return wsdm23_dict\n","\n","def load_dataset():\n","  feature_names = None\n","  label_names = None\n","\n","  file_path = '/content/drive/MyDrive/Colab Notebooks/인공지능프로젝트2/baseline_data_sp500.npy'\n","  df = read_wsdm23_file(file_path)\n","  return df\n","\n","df = load_dataset()"],"metadata":{"id":"jyTOLu59o0MO","executionInfo":{"status":"ok","timestamp":1702345849142,"user_tz":-540,"elapsed":1501,"user":{"displayName":"­최윤선 | 데이터사이언스전공 | 한양대(서울)","userId":"06275925497868952139"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["dataset = df.copy()\n","df_list = []\n","for key in dataset.keys():\n","  df_list.append(pd.DataFrame({'date':dataset[key].index, 'close':dataset[key]['close'].values, 'open':dataset[key]['open'].values, 'high':dataset[key]['high'].values,\\\n","                               'low':dataset[key]['low'].values, 'volume':dataset[key]['volume'].values, 'adjclose':dataset[key]['adjclose'].values}))\n","dataset = pd.concat(df_list, ignore_index=True)\n","dataset['date'] = pd.to_datetime(dataset['date'])\n","dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"r_dSaRS2o3JQ","executionInfo":{"status":"ok","timestamp":1702345852622,"user_tz":-540,"elapsed":3482,"user":{"displayName":"­최윤선 | 데이터사이언스전공 | 한양대(서울)","userId":"06275925497868952139"}},"outputId":"22ab0611-383a-4251-a0e2-d8477cb3345a"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["              date      close       open       high        low     volume  \\\n","0       2012-05-14  36.299999  36.360001  36.840000  35.979999   230900.0   \n","1       2012-05-15  36.750000  36.139999  37.119999  36.139999   328500.0   \n","2       2012-05-16  36.619999  36.950001  37.080002  36.549999   220300.0   \n","3       2012-05-17  34.979999  36.540001  36.540001  34.830002   418400.0   \n","4       2012-05-18  35.139999  34.900002  35.660000  34.779999   200300.0   \n","...            ...        ...        ...        ...        ...        ...   \n","1197319 2022-05-19  90.250000  91.379997  92.459999  89.540001  5234700.0   \n","1197320 2022-05-20  90.080002  90.839996  91.169998  88.430000  6585500.0   \n","1197321 2022-05-23  91.830002  90.599998  92.019997  90.000000  4701300.0   \n","1197322 2022-05-24  93.209999  91.199997  93.419998  90.470001  5932000.0   \n","1197323 2022-05-25  93.559998  93.320000  94.099998  92.589996  3742000.0   \n","\n","          adjclose  \n","0        32.237789  \n","1        32.637432  \n","2        32.521984  \n","3        31.065506  \n","4        31.207592  \n","...            ...  \n","1197319  90.250000  \n","1197320  90.080002  \n","1197321  91.830002  \n","1197322  93.209999  \n","1197323  93.559998  \n","\n","[1197324 rows x 7 columns]"],"text/html":["\n","  <div id=\"df-a7cebdc4-1f90-400e-8ac3-570c8413c720\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>close</th>\n","      <th>open</th>\n","      <th>high</th>\n","      <th>low</th>\n","      <th>volume</th>\n","      <th>adjclose</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2012-05-14</td>\n","      <td>36.299999</td>\n","      <td>36.360001</td>\n","      <td>36.840000</td>\n","      <td>35.979999</td>\n","      <td>230900.0</td>\n","      <td>32.237789</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2012-05-15</td>\n","      <td>36.750000</td>\n","      <td>36.139999</td>\n","      <td>37.119999</td>\n","      <td>36.139999</td>\n","      <td>328500.0</td>\n","      <td>32.637432</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2012-05-16</td>\n","      <td>36.619999</td>\n","      <td>36.950001</td>\n","      <td>37.080002</td>\n","      <td>36.549999</td>\n","      <td>220300.0</td>\n","      <td>32.521984</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2012-05-17</td>\n","      <td>34.979999</td>\n","      <td>36.540001</td>\n","      <td>36.540001</td>\n","      <td>34.830002</td>\n","      <td>418400.0</td>\n","      <td>31.065506</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2012-05-18</td>\n","      <td>35.139999</td>\n","      <td>34.900002</td>\n","      <td>35.660000</td>\n","      <td>34.779999</td>\n","      <td>200300.0</td>\n","      <td>31.207592</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1197319</th>\n","      <td>2022-05-19</td>\n","      <td>90.250000</td>\n","      <td>91.379997</td>\n","      <td>92.459999</td>\n","      <td>89.540001</td>\n","      <td>5234700.0</td>\n","      <td>90.250000</td>\n","    </tr>\n","    <tr>\n","      <th>1197320</th>\n","      <td>2022-05-20</td>\n","      <td>90.080002</td>\n","      <td>90.839996</td>\n","      <td>91.169998</td>\n","      <td>88.430000</td>\n","      <td>6585500.0</td>\n","      <td>90.080002</td>\n","    </tr>\n","    <tr>\n","      <th>1197321</th>\n","      <td>2022-05-23</td>\n","      <td>91.830002</td>\n","      <td>90.599998</td>\n","      <td>92.019997</td>\n","      <td>90.000000</td>\n","      <td>4701300.0</td>\n","      <td>91.830002</td>\n","    </tr>\n","    <tr>\n","      <th>1197322</th>\n","      <td>2022-05-24</td>\n","      <td>93.209999</td>\n","      <td>91.199997</td>\n","      <td>93.419998</td>\n","      <td>90.470001</td>\n","      <td>5932000.0</td>\n","      <td>93.209999</td>\n","    </tr>\n","    <tr>\n","      <th>1197323</th>\n","      <td>2022-05-25</td>\n","      <td>93.559998</td>\n","      <td>93.320000</td>\n","      <td>94.099998</td>\n","      <td>92.589996</td>\n","      <td>3742000.0</td>\n","      <td>93.559998</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1197324 rows × 7 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7cebdc4-1f90-400e-8ac3-570c8413c720')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a7cebdc4-1f90-400e-8ac3-570c8413c720 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a7cebdc4-1f90-400e-8ac3-570c8413c720');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-0c6badc1-cfd5-4b1d-a408-8993260a23f6\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0c6badc1-cfd5-4b1d-a408-8993260a23f6')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-0c6badc1-cfd5-4b1d-a408-8993260a23f6 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["train_start = datetime.strptime('2012-05-14', '%Y-%m-%d')\n","valid_start = datetime.strptime('2020-05-22', '%Y-%m-%d')\n","test_start = datetime.strptime('2021-05-22', '%Y-%m-%d')\n","test_end = datetime.strptime('2022-05-25', '%Y-%m-%d')\n","\n","train_dataset = dataset[(dataset['date'] >= train_start) & (dataset['date'] < valid_start)]\n","valid_dataset = dataset[(dataset['date'] >= valid_start) & (dataset['date'] < test_start)]\n","test_dataset = dataset[(dataset['date'] >= test_start) & (dataset['date'] <= test_end)]"],"metadata":{"id":"uqwgFbv1o4jR","executionInfo":{"status":"ok","timestamp":1702345853166,"user_tz":-540,"elapsed":548,"user":{"displayName":"­최윤선 | 데이터사이언스전공 | 한양대(서울)","userId":"06275925497868952139"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class SP500(Dataset):\n","  def __init__(self, dataset=dataset, flag='train', size=None,\n","                features='S', target='OT', scale=True, timeenc=0, freq='d'):\n","    # size [seq_len, label_len, pred_len]\n","    # info\n","    if size == None:\n","      self.seq_len = 96\n","      self.label_len = 5\n","      self.pred_len = 5\n","    else:\n","      self.seq_len = size[0]\n","      self.label_len = size[1]\n","      self.pred_len = size[2]\n","    # init\n","    assert flag in ['train', 'test', 'val']\n","    type_map = {'train': 0, 'val': 1, 'test': 2}\n","    self.set_type = type_map[flag]\n","\n","    self.features = features\n","    self.target = target\n","    self.scale = scale\n","    self.timeenc = timeenc\n","    self.freq = freq\n","    self.dataset = dataset\n","    self.__read_data__()\n","\n","  def __read_data__(self):\n","    self.scaler = StandardScaler()\n","    df_raw = dataset\n","\n","    border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]\n","    border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n","    border1 = border1s[self.set_type]\n","    border2 = border2s[self.set_type]\n","\n","    if self.features == 'M' or self.features == 'MS':\n","      cols_data = df_raw.columns[1:]\n","      df_data = df_raw[cols_data]\n","    elif self.features == 'S':\n","      df_data = df_raw[[self.target]]\n","\n","    if self.scale:\n","      train_data = df_data[border1s[0]:border2s[0]]\n","      self.scaler.fit(train_data.values)\n","      data = self.scaler.transform(df_data.values)\n","    else:\n","      data = df_data.values\n","\n","    df_stamp = df_raw[['date']][border1:border2]\n","    df_stamp['date'] = pd.to_datetime(df_stamp.date)\n","    if self.timeenc == 0:\n","      df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n","      df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n","      df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n","      df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n","      data_stamp = df_stamp.drop(['date'], axis=1).values\n","    elif self.timeenc == 1:\n","      data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n","      data_stamp = data_stamp.transpose(1, 0)\n","\n","    self.data_x = data[border1:border2]\n","    self.data_y = data[border1:border2]\n","    self.data_stamp = data_stamp\n","\n","  def __getitem__(self, index):\n","    s_begin = index\n","    s_end = s_begin + self.seq_len\n","    r_begin = s_end - self.label_len\n","    r_end = r_begin + self.label_len + self.pred_len\n","\n","    seq_x = self.data_x[s_begin:s_end]\n","    seq_y = self.data_y[r_begin:r_end]\n","    seq_x_mark = self.data_stamp[s_begin:s_end]\n","    seq_y_mark = self.data_stamp[r_begin:r_end]\n","\n","    return seq_x, seq_y, seq_x_mark, seq_y_mark\n","\n","  def __len__(self):\n","    return len(self.data_x) - self.seq_len - self.pred_len + 1\n","\n","  def inverse_transform(self, data):\n","    return self.scaler.inverse_transform(data)"],"metadata":{"id":"Kux-cnqno5YH","executionInfo":{"status":"ok","timestamp":1702345853166,"user_tz":-540,"elapsed":4,"user":{"displayName":"­최윤선 | 데이터사이언스전공 | 한양대(서울)","userId":"06275925497868952139"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def data_provider(args, flag):\n","  Data = SP500\n","  timeenc = 0 if args.embed != 'timeF' else 1\n","\n","  if flag == 'test':\n","    shuffle_flag = False\n","    drop_last = False\n","    batch_size = 1\n","    freq = args.freq\n","    dataset = test_dataset\n","  elif flag == 'val':\n","    shuffle_flag = False\n","    drop_last = True\n","    batch_size = args.batch_size\n","    freq = args.freq\n","    dataset = valid_dataset\n","  else:\n","    shuffle_flag = True\n","    drop_last = True\n","    batch_size = args.batch_size\n","    freq = args.freq\n","    dataset = train_dataset\n","\n","  data_set = Data(\n","      dataset = dataset,\n","      flag=flag,\n","      size=[args.seq_len, args.label_len, args.pred_len],\n","      features=args.features,\n","      target=args.target,\n","      timeenc=timeenc,\n","      freq=freq)\n","\n","  print(flag, len(data_set))\n","  data_loader = DataLoader(\n","      data_set,\n","      batch_size=batch_size,\n","      shuffle=shuffle_flag,\n","      num_workers=args.num_workers,\n","      drop_last=drop_last)\n","  return data_set, data_loader"],"metadata":{"id":"FJCWXHQxDUUw","executionInfo":{"status":"ok","timestamp":1702345853166,"user_tz":-540,"elapsed":3,"user":{"displayName":"­최윤선 | 데이터사이언스전공 | 한양대(서울)","userId":"06275925497868952139"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["__all__ = ['PatchTST']\n","\n","class Model(nn.Module):\n","  def __init__(self, configs, max_seq_len:Optional[int]=1024, d_k:Optional[int]=None, d_v:Optional[int]=None, norm:str='BatchNorm', attn_dropout:float=0.,\n","              act:str=\"gelu\", key_padding_mask:bool='auto',padding_var:Optional[int]=None, attn_mask:Optional[Tensor]=None, res_attention:bool=True,\n","              pre_norm:bool=False, store_attn:bool=False, pe:str='zeros', learn_pe:bool=True, pretrain_head:bool=False, head_type = 'flatten', verbose:bool=False, **kwargs):\n","\n","    super().__init__()\n","\n","    # load parameters\n","    c_in = configs.enc_in\n","    context_window = configs.seq_len\n","    target_window = configs.pred_len\n","\n","    n_layers = configs.e_layers\n","    n_heads = configs.n_heads\n","    d_model = configs.d_model\n","    d_ff = configs.d_ff\n","    dropout = configs.dropout\n","    fc_dropout = configs.fc_dropout\n","    head_dropout = configs.head_dropout\n","\n","    individual = configs.individual\n","\n","    patch_len = configs.patch_len\n","    stride = configs.stride\n","    padding_patch = configs.padding_patch\n","\n","    revin = configs.revin\n","    affine = configs.affine\n","    subtract_last = configs.subtract_last\n","\n","    decomposition = configs.decomposition\n","    kernel_size = configs.kernel_size\n","\n","\n","    # model\n","    self.decomposition = decomposition\n","    if self.decomposition:\n","      self.decomp_module = series_decomp(kernel_size)\n","      self.model_trend = PatchTST_backbone(c_in=c_in, context_window = context_window, target_window=target_window, patch_len=patch_len, stride=stride,\n","                            max_seq_len=max_seq_len, n_layers=n_layers, d_model=d_model,\n","                            n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout,\n","                            dropout=dropout, act=act, key_padding_mask=key_padding_mask, padding_var=padding_var,\n","                            attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n","                            pe=pe, learn_pe=learn_pe, fc_dropout=fc_dropout, head_dropout=head_dropout, padding_patch = padding_patch,\n","                            pretrain_head=pretrain_head, head_type=head_type, individual=individual, revin=revin, affine=affine,\n","                            subtract_last=subtract_last, verbose=verbose, **kwargs)\n","      self.model_res = PatchTST_backbone(c_in=c_in, context_window = context_window, target_window=target_window, patch_len=patch_len, stride=stride,\n","                            max_seq_len=max_seq_len, n_layers=n_layers, d_model=d_model,\n","                            n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout,\n","                            dropout=dropout, act=act, key_padding_mask=key_padding_mask, padding_var=padding_var,\n","                            attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n","                            pe=pe, learn_pe=learn_pe, fc_dropout=fc_dropout, head_dropout=head_dropout, padding_patch = padding_patch,\n","                            pretrain_head=pretrain_head, head_type=head_type, individual=individual, revin=revin, affine=affine,\n","                            subtract_last=subtract_last, verbose=verbose, **kwargs)\n","    else:\n","      self.model = PatchTST_backbone(c_in=c_in, context_window = context_window, target_window=target_window, patch_len=patch_len, stride=stride,\n","                            max_seq_len=max_seq_len, n_layers=n_layers, d_model=d_model,\n","                            n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout,\n","                            dropout=dropout, act=act, key_padding_mask=key_padding_mask, padding_var=padding_var,\n","                            attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n","                            pe=pe, learn_pe=learn_pe, fc_dropout=fc_dropout, head_dropout=head_dropout, padding_patch = padding_patch,\n","                            pretrain_head=pretrain_head, head_type=head_type, individual=individual, revin=revin, affine=affine,\n","                            subtract_last=subtract_last, verbose=verbose, **kwargs)\n","\n","\n","  def forward(self, x):           # x: [Batch, Input length, Channel]\n","    if self.decomposition:\n","      res_init, trend_init = self.decomp_module(x)\n","      res_init, trend_init = res_init.permute(0,2,1), trend_init.permute(0,2,1)  # x: [Batch, Channel, Input length]\n","      res = self.model_res(res_init)\n","      trend = self.model_trend(trend_init)\n","      x = res + trend\n","      x = x.permute(0,2,1)    # x: [Batch, Input length, Channel]\n","    else:\n","      x = x.permute(0,2,1)    # x: [Batch, Channel, Input length]\n","      x = self.model(x)\n","      x = x.permute(0,2,1)    # x: [Batch, Input length, Channel]\n","    return x"],"metadata":{"id":"fzN2btIfd9Na","executionInfo":{"status":"ok","timestamp":1702345853166,"user_tz":-540,"elapsed":3,"user":{"displayName":"­최윤선 | 데이터사이언스전공 | 한양대(서울)","userId":"06275925497868952139"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class Exp_Main(Exp_Basic):\n","  def __init__(self, args):\n","    super(Exp_Main, self).__init__(args)\n","\n","  def _build_model(self):\n","    model = Model(self.args).float()\n","\n","    if self.args.use_multi_gpu and self.args.use_gpu:\n","      model = nn.DataParallel(model, device_ids=self.args.device_ids)\n","    return model\n","\n","  def _get_data(self, flag):\n","    data_set, data_loader = data_provider(self.args, flag)\n","    return data_set, data_loader\n","\n","  def _select_optimizer(self):\n","    model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n","    return model_optim\n","\n","  def _select_criterion(self):\n","    criterion = nn.MSELoss()\n","    return criterion\n","\n","  def vali(self, vali_data, vali_loader, criterion):\n","    total_loss = []\n","    self.model.eval()\n","    with torch.no_grad():\n","      for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n","        batch_x = batch_x.float().to(self.device)\n","        batch_y = batch_y.float()\n","\n","        batch_x_mark = batch_x_mark.float().to(self.device)\n","        batch_y_mark = batch_y_mark.float().to(self.device)\n","\n","        # decoder input\n","        dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n","        dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n","        # encoder - decoder\n","        if self.args.use_amp:\n","          with torch.cuda.amp.autocast():\n","            if 'Linear' in self.args.model or 'TST' in self.args.model:\n","              outputs = self.model(batch_x)\n","            else:\n","              if self.args.output_attention:\n","                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n","              else:\n","                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n","        else:\n","          if 'Linear' in self.args.model or 'TST' in self.args.model:\n","            outputs = self.model(batch_x)\n","          else:\n","            if self.args.output_attention:\n","              outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n","            else:\n","              outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n","        f_dim = -1 if self.args.features == 'MS' else 0\n","        outputs = outputs[:, -self.args.pred_len:, f_dim:]\n","        batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n","\n","        pred = outputs.detach().cpu()\n","        true = batch_y.detach().cpu()\n","\n","        loss = criterion(pred, true)\n","\n","        total_loss.append(loss)\n","    total_loss = np.average(total_loss)\n","    self.model.train()\n","    return total_loss\n","\n","  def train(self, setting):\n","    train_data, train_loader = self._get_data(flag='train')\n","    vali_data, vali_loader = self._get_data(flag='val')\n","\n","    path = os.path.join(self.args.checkpoints, setting)\n","    if not os.path.exists(path):\n","      os.makedirs(path)\n","\n","    time_now = time.time()\n","\n","    train_steps = len(train_loader)\n","    early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)\n","\n","    model_optim = self._select_optimizer()\n","    criterion = self._select_criterion()\n","\n","    if self.args.use_amp:\n","      scaler = torch.cuda.amp.GradScaler()\n","\n","    scheduler = lr_scheduler.OneCycleLR(optimizer = model_optim,\n","                                        steps_per_epoch = train_steps,\n","                                        pct_start = self.args.pct_start,\n","                                        epochs = self.args.train_epochs,\n","                                        max_lr = self.args.learning_rate)\n","\n","    for epoch in range(self.args.train_epochs):\n","      iter_count = 0\n","      train_loss = []\n","\n","      self.model.train()\n","      epoch_time = time.time()\n","      for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n","          iter_count += 1\n","          model_optim.zero_grad()\n","          batch_x = batch_x.float().to(self.device)\n","\n","          batch_y = batch_y.float().to(self.device)\n","          batch_x_mark = batch_x_mark.float().to(self.device)\n","          batch_y_mark = batch_y_mark.float().to(self.device)\n","\n","          # decoder input\n","          dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n","          dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n","\n","          # encoder - decoder\n","          if self.args.use_amp:\n","            with torch.cuda.amp.autocast():\n","              if 'Linear' in self.args.model or 'TST' in self.args.model:\n","                outputs = self.model(batch_x)\n","              else:\n","                if self.args.output_attention:\n","                  outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n","                else:\n","                  outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n","\n","              f_dim = -1 if self.args.features == 'MS' else 0\n","              outputs = outputs[:, -self.args.pred_len:, f_dim:]\n","              batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n","              loss = criterion(outputs, batch_y)\n","              train_loss.append(loss.item())\n","          else:\n","            if 'Linear' in self.args.model or 'TST' in self.args.model:\n","              outputs = self.model(batch_x)\n","            else:\n","              if self.args.output_attention:\n","                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n","\n","              else:\n","                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark, batch_y)\n","            # print(outputs.shape,batch_y.shape)\n","            f_dim = -1 if self.args.features == 'MS' else 0\n","            outputs = outputs[:, -self.args.pred_len:, f_dim:]\n","            batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n","            loss = criterion(outputs, batch_y)\n","            train_loss.append(loss.item())\n","\n","          if (i + 1) % 100 == 0:\n","            print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n","            speed = (time.time() - time_now) / iter_count\n","            left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n","            print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n","            iter_count = 0\n","            time_now = time.time()\n","\n","          if self.args.use_amp:\n","            scaler.scale(loss).backward()\n","            scaler.step(model_optim)\n","            scaler.update()\n","          else:\n","            loss.backward()\n","            model_optim.step()\n","\n","          if self.args.lradj == 'TST':\n","            adjust_learning_rate(model_optim, scheduler, epoch + 1, self.args, printout=False)\n","            scheduler.step()\n","\n","      print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n","      train_loss = np.average(train_loss)\n","      vali_loss = self.vali(vali_data, vali_loader, criterion)\n","\n","      print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f}\".format(\n","        epoch + 1, train_steps, train_loss, vali_loss))\n","      early_stopping(vali_loss, self.model, path)\n","      if early_stopping.early_stop:\n","        print(\"Early stopping\")\n","        break\n","\n","      if self.args.lradj != 'TST':\n","        adjust_learning_rate(model_optim, scheduler, epoch + 1, self.args)\n","      else:\n","        print('Updating learning rate to {}'.format(scheduler.get_last_lr()[0]))\n","\n","    best_model_path = path + '/' + 'checkpoint.pth'\n","    self.model.load_state_dict(torch.load(best_model_path))\n","\n","    return self.model\n","\n","  def test(self, setting, test=0):\n","    test_data, test_loader = self._get_data(flag='test')\n","\n","    if test:\n","      print('loading model')\n","      self.model.load_state_dict(torch.load(os.path.join('./checkpoints/' + setting, 'checkpoint.pth')))\n","\n","    preds = []\n","    trues = []\n","    inputx = []\n","    folder_path = './test_results/' + setting + '/'\n","    if not os.path.exists(folder_path):\n","      os.makedirs(folder_path)\n","\n","    self.model.eval()\n","    with torch.no_grad():\n","      for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n","        batch_x = batch_x.float().to(self.device)\n","        batch_y = batch_y.float().to(self.device)\n","\n","        batch_x_mark = batch_x_mark.float().to(self.device)\n","        batch_y_mark = batch_y_mark.float().to(self.device)\n","\n","        # decoder input\n","        dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n","        dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n","        # encoder - decoder\n","        if self.args.use_amp:\n","          with torch.cuda.amp.autocast():\n","            if 'Linear' in self.args.model or 'TST' in self.args.model:\n","              outputs = self.model(batch_x)\n","            else:\n","              if self.args.output_attention:\n","                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n","              else:\n","                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n","        else:\n","          if 'Linear' in self.args.model or 'TST' in self.args.model:\n","            outputs = self.model(batch_x)\n","          else:\n","              if self.args.output_attention:\n","                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n","\n","              else:\n","                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n","\n","          f_dim = -1 if self.args.features == 'MS' else 0\n","          # print(outputs.shape,batch_y.shape)\n","          outputs = outputs[:, -self.args.pred_len:, f_dim:]\n","          batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n","          outputs = outputs.detach().cpu().numpy()\n","          batch_y = batch_y.detach().cpu().numpy()\n","\n","          pred = outputs  # outputs.detach().cpu().numpy()  # .squeeze()\n","          true = batch_y  # batch_y.detach().cpu().numpy()  # .squeeze()\n","\n","          preds.append(pred)\n","          trues.append(true)\n","          inputx.append(batch_x.detach().cpu().numpy())\n","          if i % 20 == 0:\n","            input = batch_x.detach().cpu().numpy()\n","            gt = np.concatenate((input[0, :, -1], true[0, :, -1]), axis=0)\n","            pd = np.concatenate((input[0, :, -1], pred[0, :, -1]), axis=0)\n","            visual(gt, pd, os.path.join(folder_path, str(i) + '.pdf'))\n","\n","    if self.args.test_flop:\n","      test_params_flop((batch_x.shape[1],batch_x.shape[2]))\n","      exit()\n","    preds = np.array(preds)\n","    trues = np.array(trues)\n","    inputx = np.array(inputx)\n","\n","    preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n","    trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])\n","    inputx = inputx.reshape(-1, inputx.shape[-2], inputx.shape[-1])\n","\n","    # result save\n","    folder_path = './results/' + setting + '/'\n","    if not os.path.exists(folder_path):\n","      os.makedirs(folder_path)\n","\n","    mae, mse, rmse, mape, mspe, rse, corr = metric(preds, trues)\n","    print('mse:{}, mae:{}, rse:{}'.format(mse, mae, rse))\n","    f = open(\"result.txt\", 'a')\n","    f.write(setting + \"  \\n\")\n","    f.write('mse:{}, mae:{}, rse:{}'.format(mse, mae, rse))\n","    f.write('\\n')\n","    f.write('\\n')\n","    f.close()\n","\n","    # np.save(folder_path + 'metrics.npy', np.array([mae, mse, rmse, mape, mspe,rse, corr]))\n","    np.save(folder_path + 'pred.npy', preds)\n","    # np.save(folder_path + 'true.npy', trues)\n","    # np.save(folder_path + 'x.npy', inputx)\n","    return preds, trues"],"metadata":{"id":"3Fus1ozHpUTb","executionInfo":{"status":"ok","timestamp":1702345853578,"user_tz":-540,"elapsed":415,"user":{"displayName":"­최윤선 | 데이터사이언스전공 | 한양대(서울)","userId":"06275925497868952139"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","  parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n","\n","  # random seed\n","  parser.add_argument('--random_seed', type=int, default=2021, help='random seed')\n","\n","  # basic config\n","  parser.add_argument('--is_training', type=bool, default=True, help='status')\n","  parser.add_argument('--model_id', type=str, default='train', help='model id')\n","  parser.add_argument('--model', type=str, default='PatchTST')\n","\n","  # data loader\n","  parser.add_argument('--data', type=str, default='SP500', help='dataset type')\n","  parser.add_argument('--dataset', default=dataset)\n","  parser.add_argument('--features', type=str, default='M',\n","                      help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n","  parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n","  parser.add_argument('--freq', type=str, default='h',\n","                      help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n","  parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n","\n","  # forecasting task\n","  parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n","  parser.add_argument('--label_len', type=int, default=5, help='start token length')\n","  parser.add_argument('--pred_len', type=int, default=5, help='prediction sequence length')\n","\n","\n","  # DLinear\n","  #parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n","\n","  # PatchTST\n","  parser.add_argument('--fc_dropout', type=float, default=0.05, help='fully connected dropout')\n","  parser.add_argument('--head_dropout', type=float, default=0.0, help='head dropout')\n","  parser.add_argument('--patch_len', type=int, default=16, help='patch length')\n","  parser.add_argument('--stride', type=int, default=8, help='stride')\n","  parser.add_argument('--padding_patch', default='end', help='None: None; end: padding on the end')\n","  parser.add_argument('--revin', type=int, default=1, help='RevIN; True 1 False 0')\n","  parser.add_argument('--affine', type=int, default=0, help='RevIN-affine; True 1 False 0')\n","  parser.add_argument('--subtract_last', type=int, default=0, help='0: subtract mean; 1: subtract last')\n","  parser.add_argument('--decomposition', type=int, default=0, help='decomposition; True 1 False 0')\n","  parser.add_argument('--kernel_size', type=int, default=25, help='decomposition-kernel')\n","  parser.add_argument('--individual', type=int, default=0, help='individual head; True 1 False 0')\n","\n","  # Formers\n","  parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n","  parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n","  parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n","  parser.add_argument('--c_out', type=int, default=7, help='output size')\n","  parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n","  parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n","  parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n","  parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n","  parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n","  parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n","  parser.add_argument('--factor', type=int, default=1, help='attn factor')\n","  parser.add_argument('--distil', action='store_false',\n","                      help='whether to use distilling in encoder, using this argument means not using distilling',\n","                      default=True)\n","  parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n","  parser.add_argument('--embed', type=str, default='timeF',\n","                      help='time features encoding, options:[timeF, fixed, learned]')\n","  parser.add_argument('--activation', type=str, default='gelu', help='activation')\n","  parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n","  parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n","\n","  # optimization\n","  parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n","  parser.add_argument('--itr', type=int, default=2, help='experiments times')\n","  parser.add_argument('--train_epochs', type=int, default=20, help='train epochs')\n","  parser.add_argument('--batch_size', type=int, default=128, help='batch size of train input data')\n","  parser.add_argument('--patience', type=int, default=100, help='early stopping patience')\n","  parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n","  parser.add_argument('--des', type=str, default='test', help='exp description')\n","  parser.add_argument('--loss', type=str, default='mse', help='loss function')\n","  parser.add_argument('--lradj', type=str, default='type3', help='adjust learning rate')\n","  parser.add_argument('--pct_start', type=float, default=0.3, help='pct_start')\n","  parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n","\n","  # GPU\n","  parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n","  parser.add_argument('--gpu', type=int, default=0, help='gpu')\n","  parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n","  parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n","  parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n","\n","  args = parser.parse_args(args=[])\n","\n","  # random seed\n","  fix_seed = args.random_seed\n","  random.seed(fix_seed)\n","  torch.manual_seed(fix_seed)\n","  np.random.seed(fix_seed)\n","\n","  args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n","\n","  if args.use_gpu and args.use_multi_gpu:\n","    args.dvices = args.devices.replace(' ', '')\n","    device_ids = args.devices.split(',')\n","    args.device_ids = [int(id_) for id_ in device_ids]\n","    args.gpu = args.device_ids[0]\n","\n","  print('Args in experiment:')\n","  print(args)\n","\n","  Exp = Exp_Main\n","\n","  if args.is_training:\n","    for ii in range(args.itr):\n","      # setting record of experiments\n","      setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'\\\n","                 .format(args.model_id, args.model, args.data, args.features, args.seq_len,\\\n","                         args.label_len, args.pred_len, args.d_model, args.n_heads, args.e_layers,\\\n","                         args.d_layers, args.d_ff, args.factor, args.embed, args.distil, args.des, ii)\n","\n","      exp = Exp(args)  # set experiments\n","      print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n","      exp.train(setting)\n","\n","      print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n","      exp.test(setting)\n","\n","      torch.cuda.empty_cache()\n","  else:\n","    ii = 0\n","    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'\\\n","               .format(args.model_id, args.model, args.data, args.features, args.seq_len,\\\n","                       args.label_len, args.pred_len, args.d_model, args.n_heads, args.e_layers,\\\n","                       args.d_layers, args.d_ff, args.factor, args.embed, args.distil, args.des, ii)\n","\n","    exp = Exp(args)  # set experiments\n","    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n","    preds, trues = exp.test(setting, test=1)\n","\n","    preds = torch.tensor(preds, dtype=torch.float32)\n","    trues = torch.tensor(trues, dtype=torch.float32)\n","    preds = preds.reshape((86280, 1)).cpu()\n","    trues = trues.reshape((86280, 1)).cpu()\n","    rankIC = spearman_corrcoef(preds, trues)\n","    print('RankIC: {}'.format(rankIC))\n","    rankIR = PearsonCorrCoef()\n","    rankIR(preds, trues)\n","    print('RankIR: {}'.format(rankIR.compute()))\n","\n","    torch.cuda.empty_cache()"],"metadata":{"id":"6mDoAagZrD82","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4e4b96b9-9662-47f4-8aeb-ed046f578b00","executionInfo":{"status":"ok","timestamp":1702346506222,"user_tz":-540,"elapsed":652646,"user":{"displayName":"­최윤선 | 데이터사이언스전공 | 한양대(서울)","userId":"06275925497868952139"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Args in experiment:\n","Namespace(random_seed=2021, is_training=True, model_id='train', model='PatchTST', data='SP500', dataset=              date      close       open       high        low     volume  \\\n","0       2012-05-14  36.299999  36.360001  36.840000  35.979999   230900.0   \n","1       2012-05-15  36.750000  36.139999  37.119999  36.139999   328500.0   \n","2       2012-05-16  36.619999  36.950001  37.080002  36.549999   220300.0   \n","3       2012-05-17  34.979999  36.540001  36.540001  34.830002   418400.0   \n","4       2012-05-18  35.139999  34.900002  35.660000  34.779999   200300.0   \n","...            ...        ...        ...        ...        ...        ...   \n","1197319 2022-05-19  90.250000  91.379997  92.459999  89.540001  5234700.0   \n","1197320 2022-05-20  90.080002  90.839996  91.169998  88.430000  6585500.0   \n","1197321 2022-05-23  91.830002  90.599998  92.019997  90.000000  4701300.0   \n","1197322 2022-05-24  93.209999  91.199997  93.419998  90.470001  5932000.0   \n","1197323 2022-05-25  93.559998  93.320000  94.099998  92.589996  3742000.0   \n","\n","          adjclose  \n","0        32.237789  \n","1        32.637432  \n","2        32.521984  \n","3        31.065506  \n","4        31.207592  \n","...            ...  \n","1197319  90.250000  \n","1197320  90.080002  \n","1197321  91.830002  \n","1197322  93.209999  \n","1197323  93.559998  \n","\n","[1197324 rows x 7 columns], features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=5, pred_len=5, fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=2, train_epochs=20, batch_size=128, patience=100, learning_rate=0.0001, des='test', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n","Use GPU: cuda:0\n",">>>>>>>start training : train_PatchTST_SP500_ftM_sl96_ll5_pl5_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n","train 8540\n","val 2876\n","Epoch: 1 cost time: 11.67071270942688\n","Epoch: 1, Steps: 66 | Train Loss: 0.1647001 Vali Loss: 0.1130115\n","Validation loss decreased (inf --> 0.113011).  Saving model ...\n","Updating learning rate to 0.0001\n","Epoch: 2 cost time: 8.046045780181885\n","Epoch: 2, Steps: 66 | Train Loss: 0.1073041 Vali Loss: 0.0860412\n","Validation loss decreased (0.113011 --> 0.086041).  Saving model ...\n","Updating learning rate to 0.0001\n","Epoch: 3 cost time: 8.900859355926514\n","Epoch: 3, Steps: 66 | Train Loss: 0.0797471 Vali Loss: 0.0790417\n","Validation loss decreased (0.086041 --> 0.079042).  Saving model ...\n","Updating learning rate to 0.0001\n","Epoch: 4 cost time: 8.390278816223145\n","Epoch: 4, Steps: 66 | Train Loss: 0.0728727 Vali Loss: 0.0797983\n","EarlyStopping counter: 1 out of 100\n","Updating learning rate to 9e-05\n","Epoch: 5 cost time: 8.395534992218018\n","Epoch: 5, Steps: 66 | Train Loss: 0.0707115 Vali Loss: 0.0769295\n","Validation loss decreased (0.079042 --> 0.076929).  Saving model ...\n","Updating learning rate to 8.1e-05\n","Epoch: 6 cost time: 8.546381711959839\n","Epoch: 6, Steps: 66 | Train Loss: 0.0677380 Vali Loss: 0.0774517\n","EarlyStopping counter: 1 out of 100\n","Updating learning rate to 7.290000000000001e-05\n","Epoch: 7 cost time: 8.599769353866577\n","Epoch: 7, Steps: 66 | Train Loss: 0.0635600 Vali Loss: 0.0779531\n","EarlyStopping counter: 2 out of 100\n","Updating learning rate to 6.561e-05\n","Epoch: 8 cost time: 8.789907693862915\n","Epoch: 8, Steps: 66 | Train Loss: 0.0635957 Vali Loss: 0.0786391\n","EarlyStopping counter: 3 out of 100\n","Updating learning rate to 5.904900000000001e-05\n","Epoch: 9 cost time: 8.577425718307495\n","Epoch: 9, Steps: 66 | Train Loss: 0.0618748 Vali Loss: 0.0805521\n","EarlyStopping counter: 4 out of 100\n","Updating learning rate to 5.3144100000000005e-05\n","Epoch: 10 cost time: 8.657464981079102\n","Epoch: 10, Steps: 66 | Train Loss: 0.0601883 Vali Loss: 0.0798092\n","EarlyStopping counter: 5 out of 100\n","Updating learning rate to 4.782969000000001e-05\n","Epoch: 11 cost time: 8.891080856323242\n","Epoch: 11, Steps: 66 | Train Loss: 0.0580743 Vali Loss: 0.0800054\n","EarlyStopping counter: 6 out of 100\n","Updating learning rate to 4.304672100000001e-05\n","Epoch: 12 cost time: 8.87233567237854\n","Epoch: 12, Steps: 66 | Train Loss: 0.0561973 Vali Loss: 0.0819554\n","EarlyStopping counter: 7 out of 100\n","Updating learning rate to 3.874204890000001e-05\n","Epoch: 13 cost time: 8.796033143997192\n","Epoch: 13, Steps: 66 | Train Loss: 0.0531206 Vali Loss: 0.0821806\n","EarlyStopping counter: 8 out of 100\n","Updating learning rate to 3.486784401000001e-05\n","Epoch: 14 cost time: 9.111849308013916\n","Epoch: 14, Steps: 66 | Train Loss: 0.0536311 Vali Loss: 0.0822046\n","EarlyStopping counter: 9 out of 100\n","Updating learning rate to 3.138105960900001e-05\n","Epoch: 15 cost time: 9.091293573379517\n","Epoch: 15, Steps: 66 | Train Loss: 0.0517566 Vali Loss: 0.0829378\n","EarlyStopping counter: 10 out of 100\n","Updating learning rate to 2.824295364810001e-05\n","Epoch: 16 cost time: 9.045167446136475\n","Epoch: 16, Steps: 66 | Train Loss: 0.0507984 Vali Loss: 0.0839317\n","EarlyStopping counter: 11 out of 100\n","Updating learning rate to 2.541865828329001e-05\n","Epoch: 17 cost time: 9.494948863983154\n","Epoch: 17, Steps: 66 | Train Loss: 0.0492138 Vali Loss: 0.0845446\n","EarlyStopping counter: 12 out of 100\n","Updating learning rate to 2.287679245496101e-05\n","Epoch: 18 cost time: 9.291327714920044\n","Epoch: 18, Steps: 66 | Train Loss: 0.0486720 Vali Loss: 0.0850252\n","EarlyStopping counter: 13 out of 100\n","Updating learning rate to 2.0589113209464907e-05\n","Epoch: 19 cost time: 9.361761093139648\n","Epoch: 19, Steps: 66 | Train Loss: 0.0481929 Vali Loss: 0.0858776\n","EarlyStopping counter: 14 out of 100\n","Updating learning rate to 1.8530201888518416e-05\n","Epoch: 20 cost time: 9.489257335662842\n","Epoch: 20, Steps: 66 | Train Loss: 0.0473394 Vali Loss: 0.0864180\n","EarlyStopping counter: 15 out of 100\n","Updating learning rate to 1.6677181699666577e-05\n",">>>>>>>testing : train_PatchTST_SP500_ftM_sl96_ll5_pl5_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","test 2876\n","mse:0.02992337755858898, mae:0.06491443514823914, rse:0.2573138177394867\n","Use GPU: cuda:0\n",">>>>>>>start training : train_PatchTST_SP500_ftM_sl96_ll5_pl5_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n","train 8540\n","val 2876\n","Epoch: 1 cost time: 8.984003067016602\n","Epoch: 1, Steps: 66 | Train Loss: 0.1662269 Vali Loss: 0.1138908\n","Validation loss decreased (inf --> 0.113891).  Saving model ...\n","Updating learning rate to 0.0001\n","Epoch: 2 cost time: 9.084482669830322\n","Epoch: 2, Steps: 66 | Train Loss: 0.1082074 Vali Loss: 0.0888789\n","Validation loss decreased (0.113891 --> 0.088879).  Saving model ...\n","Updating learning rate to 0.0001\n","Epoch: 3 cost time: 9.136426448822021\n","Epoch: 3, Steps: 66 | Train Loss: 0.0789477 Vali Loss: 0.0795781\n","Validation loss decreased (0.088879 --> 0.079578).  Saving model ...\n","Updating learning rate to 0.0001\n","Epoch: 4 cost time: 9.257606744766235\n","Epoch: 4, Steps: 66 | Train Loss: 0.0738515 Vali Loss: 0.0777543\n","Validation loss decreased (0.079578 --> 0.077754).  Saving model ...\n","Updating learning rate to 9e-05\n","Epoch: 5 cost time: 9.062216758728027\n","Epoch: 5, Steps: 66 | Train Loss: 0.0708834 Vali Loss: 0.0783133\n","EarlyStopping counter: 1 out of 100\n","Updating learning rate to 8.1e-05\n","Epoch: 6 cost time: 9.573143005371094\n","Epoch: 6, Steps: 66 | Train Loss: 0.0683969 Vali Loss: 0.0782149\n","EarlyStopping counter: 2 out of 100\n","Updating learning rate to 7.290000000000001e-05\n","Epoch: 7 cost time: 9.474910259246826\n","Epoch: 7, Steps: 66 | Train Loss: 0.0670135 Vali Loss: 0.0772768\n","Validation loss decreased (0.077754 --> 0.077277).  Saving model ...\n","Updating learning rate to 6.561e-05\n","Epoch: 8 cost time: 9.46755576133728\n","Epoch: 8, Steps: 66 | Train Loss: 0.0634911 Vali Loss: 0.0783444\n","EarlyStopping counter: 1 out of 100\n","Updating learning rate to 5.904900000000001e-05\n","Epoch: 9 cost time: 9.45418643951416\n","Epoch: 9, Steps: 66 | Train Loss: 0.0618450 Vali Loss: 0.0787039\n","EarlyStopping counter: 2 out of 100\n","Updating learning rate to 5.3144100000000005e-05\n","Epoch: 10 cost time: 9.191786050796509\n","Epoch: 10, Steps: 66 | Train Loss: 0.0594373 Vali Loss: 0.0793963\n","EarlyStopping counter: 3 out of 100\n","Updating learning rate to 4.782969000000001e-05\n","Epoch: 11 cost time: 9.742993831634521\n","Epoch: 11, Steps: 66 | Train Loss: 0.0582564 Vali Loss: 0.0799923\n","EarlyStopping counter: 4 out of 100\n","Updating learning rate to 4.304672100000001e-05\n","Epoch: 12 cost time: 9.45786190032959\n","Epoch: 12, Steps: 66 | Train Loss: 0.0561001 Vali Loss: 0.0811378\n","EarlyStopping counter: 5 out of 100\n","Updating learning rate to 3.874204890000001e-05\n","Epoch: 13 cost time: 9.67036771774292\n","Epoch: 13, Steps: 66 | Train Loss: 0.0551870 Vali Loss: 0.0812531\n","EarlyStopping counter: 6 out of 100\n","Updating learning rate to 3.486784401000001e-05\n","Epoch: 14 cost time: 9.61902666091919\n","Epoch: 14, Steps: 66 | Train Loss: 0.0531351 Vali Loss: 0.0829627\n","EarlyStopping counter: 7 out of 100\n","Updating learning rate to 3.138105960900001e-05\n","Epoch: 15 cost time: 9.495256423950195\n","Epoch: 15, Steps: 66 | Train Loss: 0.0521968 Vali Loss: 0.0843437\n","EarlyStopping counter: 8 out of 100\n","Updating learning rate to 2.824295364810001e-05\n","Epoch: 16 cost time: 9.763646602630615\n","Epoch: 16, Steps: 66 | Train Loss: 0.0497528 Vali Loss: 0.0845845\n","EarlyStopping counter: 9 out of 100\n","Updating learning rate to 2.541865828329001e-05\n","Epoch: 17 cost time: 9.364619016647339\n","Epoch: 17, Steps: 66 | Train Loss: 0.0497543 Vali Loss: 0.0847448\n","EarlyStopping counter: 10 out of 100\n","Updating learning rate to 2.287679245496101e-05\n","Epoch: 18 cost time: 9.700716257095337\n","Epoch: 18, Steps: 66 | Train Loss: 0.0489092 Vali Loss: 0.0853327\n","EarlyStopping counter: 11 out of 100\n","Updating learning rate to 2.0589113209464907e-05\n","Epoch: 19 cost time: 10.013705730438232\n","Epoch: 19, Steps: 66 | Train Loss: 0.0478879 Vali Loss: 0.0857494\n","EarlyStopping counter: 12 out of 100\n","Updating learning rate to 1.8530201888518416e-05\n","Epoch: 20 cost time: 10.077942848205566\n","Epoch: 20, Steps: 66 | Train Loss: 0.0476703 Vali Loss: 0.0862129\n","EarlyStopping counter: 13 out of 100\n","Updating learning rate to 1.6677181699666577e-05\n",">>>>>>>testing : train_PatchTST_SP500_ftM_sl96_ll5_pl5_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","test 2876\n","mse:0.030326412990689278, mae:0.06603133678436279, rse:0.259040892124176\n"]}]},{"cell_type":"code","source":["if __name__ == '__main__':\n","  parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n","\n","  # random seed\n","  parser.add_argument('--random_seed', type=int, default=2021, help='random seed')\n","\n","  # basic config\n","  parser.add_argument('--is_training', type=bool, default=False, help='status')\n","  parser.add_argument('--model_id', type=str, default='train', help='model id')\n","  parser.add_argument('--model', type=str, default='PatchTST')\n","\n","  # data loader\n","  parser.add_argument('--data', type=str, default='SP500', help='dataset type')\n","  parser.add_argument('--dataset', default=dataset)\n","  parser.add_argument('--features', type=str, default='M',\n","                      help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n","  parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n","  parser.add_argument('--freq', type=str, default='h',\n","                      help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n","  parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n","\n","  # forecasting task\n","  parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n","  parser.add_argument('--label_len', type=int, default=5, help='start token length')\n","  parser.add_argument('--pred_len', type=int, default=5, help='prediction sequence length')\n","\n","\n","  # DLinear\n","  #parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n","\n","  # PatchTST\n","  parser.add_argument('--fc_dropout', type=float, default=0.05, help='fully connected dropout')\n","  parser.add_argument('--head_dropout', type=float, default=0.0, help='head dropout')\n","  parser.add_argument('--patch_len', type=int, default=16, help='patch length')\n","  parser.add_argument('--stride', type=int, default=8, help='stride')\n","  parser.add_argument('--padding_patch', default='end', help='None: None; end: padding on the end')\n","  parser.add_argument('--revin', type=int, default=1, help='RevIN; True 1 False 0')\n","  parser.add_argument('--affine', type=int, default=0, help='RevIN-affine; True 1 False 0')\n","  parser.add_argument('--subtract_last', type=int, default=0, help='0: subtract mean; 1: subtract last')\n","  parser.add_argument('--decomposition', type=int, default=0, help='decomposition; True 1 False 0')\n","  parser.add_argument('--kernel_size', type=int, default=25, help='decomposition-kernel')\n","  parser.add_argument('--individual', type=int, default=0, help='individual head; True 1 False 0')\n","\n","  # Formers\n","  parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n","  parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n","  parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n","  parser.add_argument('--c_out', type=int, default=7, help='output size')\n","  parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n","  parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n","  parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n","  parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n","  parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n","  parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n","  parser.add_argument('--factor', type=int, default=1, help='attn factor')\n","  parser.add_argument('--distil', action='store_false',\n","                      help='whether to use distilling in encoder, using this argument means not using distilling',\n","                      default=True)\n","  parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n","  parser.add_argument('--embed', type=str, default='timeF',\n","                      help='time features encoding, options:[timeF, fixed, learned]')\n","  parser.add_argument('--activation', type=str, default='gelu', help='activation')\n","  parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n","  parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n","\n","  # optimization\n","  parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n","  parser.add_argument('--itr', type=int, default=2, help='experiments times')\n","  parser.add_argument('--train_epochs', type=int, default=20, help='train epochs')\n","  parser.add_argument('--batch_size', type=int, default=128, help='batch size of train input data')\n","  parser.add_argument('--patience', type=int, default=100, help='early stopping patience')\n","  parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n","  parser.add_argument('--des', type=str, default='test', help='exp description')\n","  parser.add_argument('--loss', type=str, default='mse', help='loss function')\n","  parser.add_argument('--lradj', type=str, default='type3', help='adjust learning rate')\n","  parser.add_argument('--pct_start', type=float, default=0.3, help='pct_start')\n","  parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n","\n","  # GPU\n","  parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n","  parser.add_argument('--gpu', type=int, default=0, help='gpu')\n","  parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n","  parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n","  parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n","\n","  args = parser.parse_args(args=[])\n","\n","  # random seed\n","  fix_seed = args.random_seed\n","  random.seed(fix_seed)\n","  torch.manual_seed(fix_seed)\n","  np.random.seed(fix_seed)\n","\n","  args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n","\n","  if args.use_gpu and args.use_multi_gpu:\n","    args.dvices = args.devices.replace(' ', '')\n","    device_ids = args.devices.split(',')\n","    args.device_ids = [int(id_) for id_ in device_ids]\n","    args.gpu = args.device_ids[0]\n","\n","  print('Args in experiment:')\n","  print(args)\n","\n","  Exp = Exp_Main\n","\n","  if args.is_training:\n","    for ii in range(args.itr):\n","      # setting record of experiments\n","      setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'\\\n","                 .format(args.model_id, args.model, args.data, args.features, args.seq_len,\\\n","                         args.label_len, args.pred_len, args.d_model, args.n_heads, args.e_layers,\\\n","                         args.d_layers, args.d_ff, args.factor, args.embed, args.distil, args.des, ii)\n","\n","      exp = Exp(args)  # set experiments\n","      print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n","      exp.train(setting)\n","\n","      print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n","      exp.test(setting)\n","\n","      torch.cuda.empty_cache()\n","  else:\n","    ii = 0\n","    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'\\\n","               .format(args.model_id, args.model, args.data, args.features, args.seq_len,\\\n","                       args.label_len, args.pred_len, args.d_model, args.n_heads, args.e_layers,\\\n","                       args.d_layers, args.d_ff, args.factor, args.embed, args.distil, args.des, ii)\n","\n","    exp = Exp(args)  # set experiments\n","    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n","    preds, trues = exp.test(setting, test=1)\n","\n","    preds = torch.tensor(preds, dtype=torch.float32)\n","    trues = torch.tensor(trues, dtype=torch.float32)\n","    preds = preds.reshape((86280, 1)).cpu()\n","    trues = trues.reshape((86280, 1)).cpu()\n","    rankIC = spearman_corrcoef(preds, trues)\n","    print('RankIC: {}'.format(rankIC))\n","    rankIR = PearsonCorrCoef()\n","    rankIR(preds, trues)\n","    print('RankIR: {}'.format(rankIR.compute()))\n","\n","    torch.cuda.empty_cache()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bTB7ssNctpDD","executionInfo":{"status":"ok","timestamp":1702346580937,"user_tz":-540,"elapsed":74721,"user":{"displayName":"­최윤선 | 데이터사이언스전공 | 한양대(서울)","userId":"06275925497868952139"}},"outputId":"457a548c-ccd8-40fb-e47f-ccacd28a294e"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Args in experiment:\n","Namespace(random_seed=2021, is_training=False, model_id='train', model='PatchTST', data='SP500', dataset=              date      close       open       high        low     volume  \\\n","0       2012-05-14  36.299999  36.360001  36.840000  35.979999   230900.0   \n","1       2012-05-15  36.750000  36.139999  37.119999  36.139999   328500.0   \n","2       2012-05-16  36.619999  36.950001  37.080002  36.549999   220300.0   \n","3       2012-05-17  34.979999  36.540001  36.540001  34.830002   418400.0   \n","4       2012-05-18  35.139999  34.900002  35.660000  34.779999   200300.0   \n","...            ...        ...        ...        ...        ...        ...   \n","1197319 2022-05-19  90.250000  91.379997  92.459999  89.540001  5234700.0   \n","1197320 2022-05-20  90.080002  90.839996  91.169998  88.430000  6585500.0   \n","1197321 2022-05-23  91.830002  90.599998  92.019997  90.000000  4701300.0   \n","1197322 2022-05-24  93.209999  91.199997  93.419998  90.470001  5932000.0   \n","1197323 2022-05-25  93.559998  93.320000  94.099998  92.589996  3742000.0   \n","\n","          adjclose  \n","0        32.237789  \n","1        32.637432  \n","2        32.521984  \n","3        31.065506  \n","4        31.207592  \n","...            ...  \n","1197319  90.250000  \n","1197320  90.080002  \n","1197321  91.830002  \n","1197322  93.209999  \n","1197323  93.559998  \n","\n","[1197324 rows x 7 columns], features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=5, pred_len=5, fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=2, train_epochs=20, batch_size=128, patience=100, learning_rate=0.0001, des='test', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n","Use GPU: cuda:0\n",">>>>>>>testing : train_PatchTST_SP500_ftM_sl96_ll5_pl5_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","test 2876\n","loading model\n","mse:0.02992337755858898, mae:0.06491443514823914, rse:0.2573138177394867\n","RankIC: tensor([0.9658])\n","RankIR: 0.9663956165313721\n"]}]}]}